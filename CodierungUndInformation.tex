\documentclass[a4paper]{scrartcl}
\pagestyle{empty}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{a4wide}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{color}

\usepackage{tikz}
\usetikzlibrary{shapes.multipart}


\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{automata}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{positioning}
\usepackage{dsfont}
\usepackage{tocloft}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{xspace}

\usepackage[colorlinks=true,linkcolor=black,a4paper=true]{hyperref}

\title{\rmfamily\Huge Information und Codierung\\[1cm]}
\subtitle{\rmfamily Vorlesungsmitschrift, Wintersemester 2009\\FAU Erlangen-Nürnberg\\[2cm]}

\author{Prof. Dr. Volker Strehl\\Christoph Rauch (Mitschrift)\\Ulrich Dorsch (\LaTeX)\\[2cm]}

%%%%%%%%%%%%%%%%%%%%%%%%% LAYOUT %%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
% Different format for headings
\def\section{\@startsection{section}{1}%
  \z@{.7\baselineskip\@plus\baselineskip}{.5\baselineskip}%
  {\large\scshape\centering}}
% This does spacing around caption.
\setlength{\abovecaptionskip}{0.5em}
\setlength{\belowcaptionskip}{0.5em}
% This does justification (left) of caption.
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{#2}%
  \ifdim \wd\@tempboxa >\hsize
    #2\par
  \else
    \global \@minipagefalse
    \hb@xt@\hsize{\box\@tempboxa\hfill}%
  \fi
  \vskip\belowcaptionskip}
\makeatother

\renewcommand\contentsname{Inhalt}
\renewcommand\cfttoctitlefont{\hfill\rmfamily\scshape\Large}
\renewcommand\cftaftertoctitle{\hfill\hfill}
\renewcommand\cftsecfont{\rmfamily\scshape}
\renewcommand\cftsecleader{\cftdotfill{2}}
\renewcommand\cftsecpagefont{\rmfamily}

\theoremstyle{plain}
\newtheorem{theorem}{Satz}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Korollar}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheoremstyle{examplestyle}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\normalfont}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.\hfill}         % HEADPUNCT
  {\newline} %5pt plus 1pt minus 1pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC

\theoremstyle{examplestyle}
\newtheorem{example}{Beispiel}[section]

%%%%%%%%%%%%%%%%%%%%%%%% COMMANDS %%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\WK{Wahrscheinlichkeit\xspace}
\newcommand\WV{Wahrscheinlichkeitsverteilung\xspace}
\newcommand\ZV{Zufallsvariable\xspace}
\newcommand\coding{\ensuremath{\mathcal C}\xspace}
\newcommand{\vect}[1]{\ensuremath{\bf #1}\xspace}
%\DeclareMathOperator\dim{dim}

%%%%%%%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Grundlagen}

\begin{example}[Optimale Wetten bei Pferderennen ({\scshape Kelly}-gambling)]
	\begin{itemize}
		\item Pferde $1, 2, \ldots, m$
		\item \WV $\mathbf p = \left(p_1, p_2, \ldots, p_m\right)$, $p_i$ ist die \WK dass Pferd $i$ gewinnt.
		\item Auszahlung (``payoff'') $\mathbf a = \left(a_1, a_2, \ldots, a_m\right)$, $a_i$ Gewinn, falls Pferd $i$ gewinnt.
		\item das Protfolio $\mathbf b = \left(b_1, b_2, \ldots, b_m\right)$ ist eine \WV, wobei $b_i$ der Anteil des Gesamteinsatzes ist, der auf Pferd $i$ gesetzt wird.
	\end{itemize}
	Was ist das optimale $\mathbf b$? Was hat das mit Entropie zu tun?
\end{example}

\begin{definition}
	Fünf wichtige numerische Größen der Informationstheorie:
	\begin{enumerate}
	  \item Entropie $H$ für \WV $\mathbf p = \left( p_1, \dots, p_m \right)$:
		\[ H\left( \mathbf p \right) = -\sum_{i=1}^{m} p_i\log p_i \]
	  \item Gegenseitige Information $I$ zweier {\ZV}n $X,Y$ mit Werten in $\left\{ 1, \dots, n \right\}$,
		gemeinsamer Verteilung $p_{X,Y}\left( i,j \right) = P\left( X=i,Y=j \right)$ sowie Randverteilung $p_X\left( i \right) = \sum\limits_{j=1}^n p_{X,Y}\left( i, j \right)$
		und $p_Y\left( j \right) = \sum\limits_{i=1}^n p_{X,Y}\left( i, j \right)$:
		\begin{align*}
		I\left( X, Y \right) & = H\left( X \right) + H\left( Y \right) - H\left( X,Y \right) \\
	  		& = \sum_{i,j}p_{X,Y}\left( i,j \right)\log \frac{p_{X,Y}\left( i,j \right)}{p_X\left( i \right)p_Y\left( j \right)} \\
		  	& = \sum_{i,j}p_{X,Y}\left( i,j \right)\log \frac{P\left( Y=j \mid X=i \right)}{p_Y\left( j \right)} \\
		\end{align*}
		$P\left( Y=j \mid X=i \right)$ ist die bedingte \WK des Ereignisses $Y=j$ wenn $X=i$ gilt, und kann in diesem informationstheoretischen Kontext als die \WK angesehen werden
		dass unter der Bedingung, dass Zeichen $i$ \"uber einen verrauschten Kanal gesendet wird, das Zeichen $j$ empfangen wird.
	  \item Kapzit\"at $C$ eines Kanals:
		\[ C = \max_{p_{X}} I\left( X,Y \right) \]
	  \item relative Entropie $D$ ({\scshape Kullback-Leibler}-Distanz) f\"ur zwei {\WV}en $\mathbf p = \left( p_1, \dots,p_n \right)$, $\mathbf q = \left( q_1, \dots,q_m \right)$:
		\[ D\left( p || q \right) = \sum_{1 \leq i \leq m} p_i \log \frac{p_i}{q_i} \geq 0 \; \left( = 0 \Leftrightarrow p=q \right)\]
		$I\left( X,Y \right)$ ist die relative Entropie zwischen gemeinsamer Verteilung $p_{X,Y}$ und Produktverteilung $p_Xp_Y$.
	  \item Verdopplungsrate $W$ von \WV $\mathbf p = \left( p_1, \dots,p_m \right)$:
		\[ W = \max_{\mathbf b} \sum_{i=1}^m p_i \log\left( b_i a_i \right) \overset{!}{=} \sum_{i=1}^m p_i \log a_i - H\left( \mathbf p \right) \]
	\end{enumerate}
\end{definition}

\begin{example}[Pferderennen (cont'd)]
  Sei $X$ eine \ZV mit \WK $p_i$ f\"ur das Ereignis $\left[ X = i \right]$.
  Seien $X_1,\dots,X_n$ unabh\"angige \ZV mit der gleichen \WV wie $X$. Sei $k^{\left( 0 \right)}$ das Kapital vor der ersten Runde des Pferderennens.
  \begin{itemize}
	\item Kapital nach einem Rennen: $k^{\left( 1 \right)} = \underbrace{b\left( X_1 \right) \cdot a\left( X_1 \right)}_{s\left( X_1 \right)}\cdot{k^{\left( 0 \right)}}$
	\item Kapital nach $n$ Rennen: $k^{\left( n \right)} = k^{\left( 0 \right)} \cdot \displaystyle{\prod_{j=1}^n} s\left( X_j \right) $
  \end{itemize}
  Das Ziel ist es das Portfolio $\mathbf b$ so zu w\"ahlen, dass der Erwartungswert $E\left[ k^{\left( n \right)} \right]$ maximal wird, dabei bezeichnet $S_n$ den Faktor der
  Vervielfachung des Einsatzkapitals:
  \[ \frac{1}{n} \log S_n = \frac{1}{n}\sum_{j=1}^n\log s\left( X_j \right) \underset{n \rightarrow \infty}\rightarrow
		E_\mathbf{p}\left[ \log S\left( X \right) \right] \quad \left( = W\left( \mathbf b, \mathbf p \right) \right)\]
	\[ \Rightarrow S_n \approx 2^{n W\left( \mathbf b, \mathbf p \right)} \]
	Eine Verdopplung des Einsatzkapitals erh\"alt man also wenn:
	\[ S_n \approx 2 \Rightarrow n = \frac{1}{W\left( \mathbf b, \mathbf p \right)} \]
	mit
	\[ W\left( \mathbf b, \mathbf p \right) = \sum_{i=1}^n p_i\log\left( b_i\cdot a_i \right) \]
	Die optimale Strategie $\mathbf b$ erh\"alt man durch die Maximierung:
	\[ W^\ast \left( \mathbf p \right) = \max_{\mathbf b} \sum_{i=1}^m p_i \log \left( b_i a_i \right) = \max_{\mathbf b} W\left( \mathbf b, \mathbf p \right) \]
	mit
	\[ W\left( \mathbf b, \mathbf p \right) = \sum_{i=1}^m p_i \log\left( b_ia_i \right) = \underbrace{\sum_{i=1}^m\log a_i - H\left( \mathbf p \right)}_{W^\ast\left( \mathbf p \right)}
	- \underbrace{D\left( \mathbf p\, ||\, \mathbf b \right)}_{\geq 0} \]
	Das Maximum von $W\left( \mathbf b,\mathbf p \right)$ erhält man also wenn $D\left( \mathbf p\, ||\, \mathbf b \right)$ minimal ist, i.e.\@ $\mathbf p = \mathbf b$.
\end{example}

\pagebreak

\begin{definition}[\textsc{Shannon}-Schema]
  Nachrichtenübertragung über gestörte Kanäle mittels Blockcodierung:
  \begin{figure}[h!]
	\begin{center}
          \begin{tikzpicture}[every text node part/.style={align=center},every
            node/.style={text width=3cm}]
	\coordinate (q) at (0,4);
	\coordinate (qc) at (4,4);
	\coordinate (kc) at (8,4);
        \coordinate (kck) at (12,4);
	\coordinate (k) at (12,2);
        \coordinate (kkd) at (12,0);
	\coordinate (kd) at (8,0);
	\coordinate (zd) at (4,0);
	\coordinate (z) at (0,0);

	\node at (q) {Quelle};

        \node at (qc) {Quellcodierung};
        \node[below=15pt] at (qc) {\tiny Beseitigen von Redundanz, Anpassen an Kanal};

        \node at (kc) {Kanalcodierung};
        \node[below=15pt] at (kc) {\tiny kontrolliertes Hinzufügen von Redundanz,
        $c:A^k \rightarrow A^n$};

        \node at (k) {Kanal};
        \node[below=5pt] at (k) {\tiny fehlerbehaftet};

        \node at (kd) {Kanaldecodierung};
        \node[below=5pt] at (kd) {\tiny $D: A^n \rightarrow \coding$};

	\node at (zd) {Zieldecodierung};

	\node at (z) {Ziel};

	%\filldraw (q) circle [radius=2pt];
        \draw[->,shorten >=1.5cm,shorten <=1.5cm] (q) -- (qc);
	%\filldraw (qc) circle [radius=2pt];
        \draw[->,shorten >=1.5cm,shorten <=1.5cm] (qc) -- node[above] {\tiny $\vect u \in A^k$} (kc);
	%\filldraw (kc) circle [radius=2pt];
        \draw[->,shorten >=0.5cm,shorten <=1.5cm] (kc) -- node[above right] {\tiny
          ${\vect a}\in A^n$} (kck) -- (k);
	%\filldraw (k) circle [radius=2pt];
        \draw[->,shorten >=1.6cm,shorten <=0.5cm] (k) -- (kkd) -- node[below right] {\tiny
          ${\vect b}\in A^n$} (kd);
	%\filldraw (kd) circle [radius=2pt];
          \draw[->,shorten >=1.5cm,shorten <=1.6cm] (kd) -- node[above=0.5cm] {\tiny $\hat{\vect u} =
            c'(\hat{\vect a}) \leftarrow \hat{\vect
            a}\in
          \coding$} (zd);
	%\filldraw (zd) circle [radius=2pt];
        \draw[->,shorten >=1.5cm,shorten <=1.5cm] (zd) -- (z);
	%\filldraw (z) circle [radius=2pt];

        \draw[thin,gray,dashed] (-0.7,3.5) -- (5.4,3.5) -- (5.4,4.5) -- (-0.7,4.5) --
        (-0.7,3.5);
        \draw[thin,gray,dashed] (-0.7,-0.5) -- (5.4,-0.5) -- (5.4,0.5) -- (-0.7,0.5) --
        (-0.7,-0.5);
        \draw[thin,->,gray,dashed,shorten >=0.6cm,shorten <=0.6cm] (q) -- (z);
	\end{tikzpicture}
	\end{center}
	\end{figure}
\end{definition}

\begin{minipage}[t]{0.5\textwidth}\raggedright
  \begin{itemize}
     \item channel alphabet $A$, $\left|A\right| = q$ 
    \item  source ($A^k$, uniform distribution) 
      \item  $d$: metric on $A^k$: $d({\vect a}, {\vect b}) = \#j(a_j\neq b_j)$ 
      \item  coding: $c:A^k \rightarrow A^n$ injective\\
       Code: $\mathcal C = \left\{ c({\vect a}) \mid {\vect a}\in A^k \right\}
          \subseteq A^n$ 
          \item  channel: stochastic matrix $(p({\vect b}\mid{\vect a}))_{ {\vect a},{\vect
            b}\in A^n }$ 
      \item  decoding: $D:A^n \rightarrow \coding = c[A^k]$ \\
      $D_{ml}$: maximum likelihood, 
      $D_{map}$: maximum a posteriori,
      $D_{nc}$: nearest codeword
    \item rate of a code: $\frac{k}{n}=\frac{1}{n}\log_q \vert\coding\vert$
    \item minimum distance of \coding: $d_{\min}(\coding) = \underset{\vect a,\vect
      b\in\coding,\vect a \neq\vect b}{\min}(\vect a,\vect b)$
  \end{itemize}
\end{minipage}\hfill\noindent
\begin{minipage}[t]{0.5\textwidth}\raggedright
\begin{itemize}
  \item $A$ is a group or finite field

  \item $d({\vect a}, {\vect b}) = d({\vect a}-{\vect b}, {\vect 0}) = \|{\vect a} - {\vect
    b}\|$, $\|\vect a\| = \#j(a_j \neq 0)$
  \item $c$ linear between vectorspaces $A^k\rightarrow A^n$\\
   $\mathcal C\subseteq A^n$ subspace of dimention $k$
  \item channel memoryless: $p\left( {\vect b}\mid{\vect a} \right) =
        \prod\limits_{j=1}^n p\left( b_j | a_j \right)$\\
      channel additive: $p\left( {\vect b}\mid{\vect a} \right) = p( \underbrace{{\vect
        b}-{\vect a}}_{\text{error vector}}\mid {\vect 0} )$
  $p({\vect b}\mid{\vect a}) = \prod\limits_{j=1}^n p_{err}(b_j - a_j)$\\
  $p_{err}$: error probability distribution on $A$ 
  $p_{err}(e) = p\left( \vect e \mid \vect 0 \right)$
  \item $k = \dim \coding$
  \item if \coding is linear: $d_{\min}(\coding) = \underset{\vect a \in \coding, \vect a \neq
    \vect 0}{\min} \Vert
    \vect a \Vert$ minimum weight
\end{itemize}
\end{minipage}

Notation:

\coding is $(n, k, d)$-Code over $q$-ary alphabet: $\coding\subseteq A^n$,
$k=\vert\coding\vert$, $d$: minimum distance.

\coding is $\left[ n,k,d \right]$-Code: \coding is linear, $\coding\subseteq A^n$ as a linear
subspace, $k=\dim\coding$, $d$: minimum weight.
\end{document}
